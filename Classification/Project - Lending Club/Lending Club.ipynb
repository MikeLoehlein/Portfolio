{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Introduction:\n",
    "     Source Data from: https://www.kaggle.com/wendykan/lending-club-loan-data/data\n",
    "    \n",
    "     The goal of this script is to develop a model to estimate the probability of charge off given a loans origination\n",
    "     characteristics. Only information available at origination will be used. Given that the portfolio is a mix of \n",
    "     current, paid off, charged off, and delinquent loans; all loans that are current will assume to stay current for the \n",
    "     life of the loan and all loans that are delinquent will be assumed to result in a charge-off.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Set Column Width to View all Columns in the Dataset\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load user defined functions\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    '''Simple Function that prints the select functions\n",
    "        Paramters:\n",
    "        - true_labels: the 'true' or actual labels\n",
    "        - predicted_labels: the predicted labels\n",
    "    '''\n",
    "    \n",
    "    print('Accuracy:' + str(np.round(metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),2)))\n",
    "    \n",
    "    print('Precision:' + str(np.round(metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),2)))\n",
    "    \n",
    "    print('Recall:' + str(np.round(metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),2)))\n",
    "    \n",
    "    print('F1 Score:' + str(np.round(metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),2)))\n",
    "    print()\n",
    "                        \n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    '''\n",
    "    Function to estimate a classifier and print out the model fit metrics\n",
    "    Paramters:\n",
    "        - classifier: a sklearn algorithm object\n",
    "        - train_features: the estimation features\n",
    "        - train_labels: the estimation labels\n",
    "        - test_features: the testing features\n",
    "        - test_labels: the testing labels\n",
    "    '''\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    # predict using model\n",
    "    predictions_insample = classifier.predict(train_features) \n",
    "    predictions = classifier.predict(test_features) \n",
    "    \n",
    "    # evaluate model prediction performance   \n",
    "    print(\"In of Sample Metrics\")\n",
    "    get_metrics(true_labels=train_labels, predicted_labels=predictions_insample)\n",
    "    \n",
    "    print(\"Out of Sample Metrics\")\n",
    "    get_metrics(true_labels=test_labels, predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "dta = pd.read_csv(\"C:/Personal/Kaggle/lending-club-loan-data/loan.csv\",dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print  Column Names\n",
    "print('Column Names are:')\n",
    "print(dta.columns)\n",
    "\n",
    "#Print Dataset Shape\n",
    "print()\n",
    "print(\"Shape is: \" + str(dta.shape))\n",
    "\n",
    "#View top five records\n",
    "print()\n",
    "print(\"Data frame head is: \")\n",
    "print(dta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the percent of records with missing for each column\n",
    "null_rate = dta.isnull().sum()/len(dta) \n",
    "print(null_rate)\n",
    "print() \n",
    "\n",
    "#Create a list of columns to keep with no more than 25% missing variables\n",
    "null_index = list(null_rate[null_rate < .75].index) \n",
    "print(null_index)\n",
    "print()\n",
    "\n",
    "#Remove  columns that are not needed or going to be used given that they are related to post origination performance\n",
    "null_index.remove('id')\n",
    "null_index.remove('member_id')\n",
    "null_index.remove('emp_title')\n",
    "null_index.remove('url')\n",
    "null_index.remove('title')\n",
    "null_index.remove('pymnt_plan')\n",
    "null_index.remove('zip_code')\n",
    "null_index.remove('initial_list_status')\n",
    "null_index.remove('out_prncp') \n",
    "null_index.remove('out_prncp_inv')\n",
    "null_index.remove('total_pymnt')\n",
    "null_index.remove('total_rec_int')\n",
    "null_index.remove('total_rec_prncp')\n",
    "null_index.remove('total_pymnt_inv')\n",
    "null_index.remove('total_rec_late_fee')\n",
    "null_index.remove('recoveries')\n",
    "null_index.remove('collection_recovery_fee')\n",
    "null_index.remove('last_pymnt_d')\n",
    "null_index.remove('last_pymnt_amnt')\n",
    "null_index.remove('next_pymnt_d')\n",
    "null_index.remove('last_credit_pull_d')\n",
    "null_index.remove('mths_since_last_delinq')\n",
    "null_index.remove('issue_d') \n",
    "null_index.remove('earliest_cr_line') \n",
    "null_index.remove('policy_code') \n",
    "null_index.remove('sub_grade') \n",
    "null_index.remove('application_type') \n",
    "null_index.remove('int_rate') \n",
    "null_index.remove('installment') \n",
    "\n",
    "#Subset the data to keep only selected columns\n",
    "dta = dta[null_index]\n",
    "\n",
    "#Print column names\n",
    "print(dta.columns)\n",
    "print()\n",
    "\n",
    "#Print data head\n",
    "print(dta.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data - Remove Rows with Missing Data\n",
    "\n",
    "#For each column, print the percent of records with NAs\n",
    "print(dta.isnull().sum()/len(dta))\n",
    "\n",
    "#Remove Records with  NA's\n",
    "print('shape origin - with NAs: ' + str(dta.shape))\n",
    "dta.dropna(axis=0, how='any', inplace=True)\n",
    "print('shape clean - without NAs: ' + str(dta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Lists of Character Categories\n",
    "print(dta.term.value_counts()) #Need to Create Dummy Variables For Modeling, '36 months' is reference class\n",
    "print(dta.grade.value_counts(())) #Need to Create Dummy Variables For Modeling, 'A' is reference class\n",
    "print(dta.emp_length.value_counts(())) #Recode NA and <1 year to 0; '0' is reference class\n",
    "print(dta.home_ownership.value_counts(())) #Remove Other/None/Any and set as NA; 'mortgage' is reference class\n",
    "print(dta.verification_status.value_counts(())) #\"Source Verified\" is reference class\n",
    "print(dta.purpose.value_counts(())) #Recode to other if count is < 10,000. \"debt_consolidation\" is refernece class\n",
    "print(dta.addr_state.value_counts(())) #\"CA\" is reference class\n",
    "\n",
    "print(dta.loan_status.value_counts(())) #Dependent Variable, reclassify so it is only pay off \n",
    "        #and charge off and then encode so charge off = 1 and pay off = 0\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record Values in Columns to Standardise and reduce the number of levels as needed\n",
    "##### emp_length #####\n",
    "mask = dta.emp_length.isin(['n/a'])\n",
    "column_name = 'emp_length'\n",
    "dta.loc[mask, column_name] = '< 1 year'\n",
    "print(dta.emp_length.value_counts(()))\n",
    "print()\n",
    "\n",
    "##### home_ownership #####\n",
    "mask = dta.home_ownership.isin(['OTHER','NONE','ANY'])\n",
    "column_name = 'home_ownership'\n",
    "dta.loc[mask, column_name] = 'OTHER'\n",
    "print(dta.home_ownership.value_counts(()))\n",
    "print()\n",
    "\n",
    "##### purpose #####\n",
    "counts = dta.purpose.value_counts()\n",
    "counts = list(counts.loc[counts < 10000].index.values)\n",
    "mask = dta.purpose.isin(counts)\n",
    "column_name = 'purpose'\n",
    "dta.loc[mask, column_name] = \"other\"\n",
    "print(dta.purpose.value_counts(()))\n",
    "print()\n",
    "\n",
    "##### loan_status #####\n",
    "chargeoff = ['Charged Off','Late (31-120 days)','In Grace Period','Late (16-30 days)','Default']\n",
    "mask = dta.loan_status.isin(chargeoff)\n",
    "column_name = 'loan_status'\n",
    "dta.loc[mask, column_name] = \"Default\"\n",
    "dta.loc[~mask, column_name] = \"PayOff\"\n",
    "\n",
    "column_name = 'y'\n",
    "dta.loc[mask, column_name] = 1\n",
    "dta.loc[~mask, column_name] = 0\n",
    "print(dta.y.value_counts(()))\n",
    "print()\n",
    "\n",
    "##### Grade #####\n",
    "labeler = LabelEncoder()\n",
    "labeler = labeler.fit(dta.grade)\n",
    "\n",
    "y2 = labeler.transform(dta.grade)\n",
    "y2 = pd.DataFrame(y2, columns=['y2'])\n",
    "dta = dta.reset_index()\n",
    "dta = pd.concat([dta, y2], axis=1)\n",
    "\n",
    "print(dta.grade.value_counts(()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast Columns to Correct Numeric Formats and Encodings; Strings can remain as objects\n",
    "#Print Data Types\n",
    "print(dta.dtypes)\n",
    "\n",
    "#Print Head\n",
    "print(dta.head(2))\n",
    "\n",
    "#To Numeric\n",
    "dta.loan_amnt = pd.to_numeric(dta.loan_amnt)\n",
    "dta.funded_amnt = pd.to_numeric(dta.funded_amnt)\n",
    "dta.funded_amnt_inv = pd.to_numeric(dta.funded_amnt_inv)\n",
    "dta.annual_inc = pd.to_numeric(dta.annual_inc)\n",
    "dta.dti = pd.to_numeric(dta.dti)\n",
    "dta.delinq_2yrs = pd.to_numeric(dta.delinq_2yrs)\n",
    "dta.inq_last_6mths = pd.to_numeric(dta.inq_last_6mths)\n",
    "dta.open_acc = pd.to_numeric(dta.open_acc)\n",
    "dta.pub_rec = pd.to_numeric(dta.pub_rec)\n",
    "dta.revol_util = pd.to_numeric(dta.revol_util)\n",
    "dta.revol_bal = pd.to_numeric(dta.revol_bal)\n",
    "dta.total_acc = pd.to_numeric(dta.total_acc)\n",
    "dta.collections_12_mths_ex_med = pd.to_numeric(dta.collections_12_mths_ex_med)\n",
    "dta.acc_now_delinq = pd.to_numeric(dta.acc_now_delinq)\n",
    "dta.tot_coll_amt = pd.to_numeric(dta.tot_coll_amt)\n",
    "dta.tot_cur_bal = pd.to_numeric(dta.tot_cur_bal)\n",
    "dta.total_rev_hi_lim = pd.to_numeric(dta.total_rev_hi_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bivariate Exporation\n",
    "#loan_status by ach potential explanatory variable\n",
    "print('Mean acc_now_delinq by loan_status')\n",
    "print(dta.groupby('loan_status').acc_now_delinq.mean())\n",
    "print()\n",
    "\n",
    "print('Mean annual_inc by loan_status')\n",
    "print(dta.groupby('loan_status').annual_inc.mean())\n",
    "print()\n",
    "\n",
    "print('Mean collections_12_mths_ex_med by loan_status')\n",
    "print(dta.groupby('loan_status').collections_12_mths_ex_med.mean())\n",
    "print()\n",
    "\n",
    "print('Mean delinq_2yrs by loan_status')\n",
    "print(dta.groupby('loan_status').delinq_2yrs.mean())\n",
    "print()\n",
    "\n",
    "print('Mean dti by loan_status')\n",
    "print(dta.groupby('loan_status').dti.mean())\n",
    "print()\n",
    "\n",
    "print('Mean funded_amnt by loan_status')\n",
    "print(dta.groupby('loan_status').funded_amnt.mean())\n",
    "print()\n",
    "\n",
    "print('Mean funded_amnt_inv by loan_status')\n",
    "print(dta.groupby('loan_status').funded_amnt_inv.mean())\n",
    "print()\n",
    "\n",
    "print('Mean loan_amnt by loan_status')\n",
    "print(dta.groupby('loan_status').loan_amnt.mean())\n",
    "print()\n",
    "\n",
    "print('Mean inq_last_6mths by loan_status')\n",
    "print(dta.groupby('loan_status').inq_last_6mths.mean())\n",
    "print()\n",
    "\n",
    "print('Mean open_acc by loan_status')\n",
    "print(dta.groupby('loan_status').open_acc.mean())\n",
    "print()\n",
    "\n",
    "print('Mean pub_rec by loan_status')\n",
    "print(dta.groupby('loan_status').pub_rec.mean())\n",
    "print()\n",
    "\n",
    "print('Mean revol_bal by loan_status')\n",
    "print(dta.groupby('loan_status').revol_bal.mean())\n",
    "print()\n",
    "\n",
    "print('Mean revol_util by loan_status')\n",
    "print(dta.groupby('loan_status').revol_util.mean())\n",
    "print()\n",
    "\n",
    "print('Mean tot_coll_amt by loan_status')\n",
    "print(dta.groupby('loan_status').tot_coll_amt.mean())\n",
    "print()\n",
    "\n",
    "print('Mean tot_cur_bal by loan_status')\n",
    "print(dta.groupby('loan_status').tot_cur_bal.mean())\n",
    "print()\n",
    "\n",
    "print('Mean total_acc by loan_status')\n",
    "print(dta.groupby('loan_status').total_acc.mean())\n",
    "print()\n",
    "\n",
    "print('Mean total_rev_hi_lim by loan_status')\n",
    "print(dta.groupby('loan_status').total_rev_hi_lim.mean())\n",
    "print()\n",
    "\n",
    "print('Term by loan_status')\n",
    "print(dta.groupby(['term']).loan_status.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print('verification_status by loan_status')\n",
    "print(dta.groupby(['verification_status']).loan_status.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print('addr_state by loan_status')\n",
    "print(dta.groupby(['addr_state']).loan_status.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print('emp_length by loan_status')\n",
    "print(dta.groupby(['emp_length']).loan_status.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print('home_ownership by loan_status')\n",
    "print(dta.groupby(['home_ownership']).loan_status.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print('purpose by loan_status')\n",
    "print(dta.groupby(['purpose']).loan_status.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print('grade by loan_status')\n",
    "print(dta.groupby(['grade']).loan_status.value_counts(normalize=True))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dummy Variables for Categorical Data\n",
    "dta_dummied = pd.get_dummies(dta, \n",
    "                             drop_first=True, \n",
    "                             prefix=['term','emp_length','home_ownership','verification_status','purpose','addr_state'],\n",
    "                             columns=['term','emp_length','home_ownership','verification_status','purpose','addr_state']                            )\n",
    "\n",
    "#Remove Source Dependent Variables\n",
    "dta_dummied.drop(['loan_status','grade'], axis=1, inplace=True)\n",
    "print(dta_dummied.shape)\n",
    "print()\n",
    "\n",
    "#Remove any missing\n",
    "print('Pre Cleaning Count: ' + str(dta_dummied.shape))\n",
    "dta_dummied.dropna(axis=0, how='any', inplace=True)\n",
    "print('Post Cleaning Count: ' + str(dta_dummied.shape))\n",
    "print()\n",
    "\n",
    "y = dta_dummied.y\n",
    "y2 = dta_dummied.y2\n",
    "print(y[:5])\n",
    "print(y2[:5])\n",
    "\n",
    "X = dta_dummied.drop(['y','y2'], axis=1)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data in to Train and Test Sets - Target Var = Default\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "#Print Shapes for Training data for Target Var Default\n",
    "print('Shape of Default Data')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print()\n",
    "\n",
    "#Split Data in to Train and Test Sets - Targe Var = Grade\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, test_size=0.10, random_state=42)\n",
    "\n",
    "print('Shape of Grade Data')\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model for Default Estimate\n",
    "print('Dependent Variable - Loand Status (Default = 1)')\n",
    "lgr_default = LogisticRegression(penalty='l2', C=1, class_weight='balanced')\n",
    "\n",
    "lgr_default_pred = train_predict_evaluate_model(classifier=lgr_default,\n",
    "                                           train_features=X_train,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=X_test,\n",
    "                                           test_labels=y_test)\n",
    "\n",
    "y_predicted = lgr_default.predict(X_test)\n",
    "\n",
    "labels = list(set(y_test))\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted, labels)\n",
    "cm2 = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(cm2)\n",
    "cm = cm / cm.astype(np.float).sum(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dependent Variable - Grade, predicted probability of default')\n",
    "lgr_grade = LogisticRegression(penalty='l2', C=1, class_weight='balanced')\n",
    "\n",
    "x_trainp = lgr_default.predict_proba(X_train2)\n",
    "x_testp = lgr_default.predict_proba(X_test2)\n",
    "\n",
    "lgr_grade_pred = train_predict_evaluate_model(classifier=lgr_grade,\n",
    "                                           train_features=x_trainp,\n",
    "                                           train_labels=y_train2,\n",
    "                                           test_features=x_testp,\n",
    "                                           test_labels=y_test2)\n",
    "\n",
    "y_predicted = lgr_grade.predict(x_testp)\n",
    "train = labeler.inverse_transform(y_test2)\n",
    "test = labeler.inverse_transform(y_predicted)\n",
    "\n",
    "labels = list(set(train))\n",
    "cm = metrics.confusion_matrix(train, test, labels)\n",
    "cm2 = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(cm2)\n",
    "cm = cm / cm.astype(np.float).sum(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dependent Variable - Grade, full origination data')\n",
    "lgr_grade = LogisticRegression(penalty='l2', C=1, class_weight='balanced')\n",
    "\n",
    "lgr_grade_pred = train_predict_evaluate_model(classifier=lgr_grade,\n",
    "                                           train_features=X_train2,\n",
    "                                           train_labels=y_train2,\n",
    "                                           test_features=X_test2,\n",
    "                                           test_labels=y_test2)\n",
    "\n",
    "y_predicted = lgr_grade.predict(X_test2)\n",
    "train = labeler.inverse_transform(y_test2)\n",
    "test = labeler.inverse_transform(y_predicted)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = list(set(train))\n",
    "cm = metrics.confusion_matrix(train, test, labels)\n",
    "cm2 = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(cm2)\n",
    "cm = cm / cm.astype(np.float).sum(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest - Grade, full origination data')\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True, n_jobs=2, n_estimators=250, \n",
    "                                        max_features='sqrt', criterion='gini', max_depth=None, \n",
    "                                        min_samples_split=10, min_samples_leaf=5, max_leaf_nodes=None)\n",
    "\n",
    "rf_grade_pred = train_predict_evaluate_model(classifier=rf,\n",
    "                                           train_features=X_train2,\n",
    "                                           train_labels=y_train2,\n",
    "                                           test_features=X_test2,\n",
    "                                           test_labels=y_test2)\n",
    "\n",
    "y_predicted = rf.predict(X_test2)\n",
    "train = labeler.inverse_transform(y_test2)\n",
    "test = labeler.inverse_transform(y_predicted)\n",
    "\n",
    "labels = list(set(train))\n",
    "cm = metrics.confusion_matrix(train, test, labels)\n",
    "cm2 = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(cm2)\n",
    "cm = cm / cm.astype(np.float).sum(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest - Default, full origination data')\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True, n_jobs=2, n_estimators=250, \n",
    "                                        max_features='sqrt', criterion='gini', max_depth=None, \n",
    "                                        min_samples_split=10, min_samples_leaf=5, max_leaf_nodes=None)\n",
    "\n",
    "rf_grade_pred = train_predict_evaluate_model(classifier=rf,\n",
    "                                           train_features=X_train,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=X_test,\n",
    "                                           test_labels=y_test)\n",
    "\n",
    "y_predicted = rf.predict(X_test2)\n",
    "\n",
    "labels = list(set(y_test))\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted, labels)\n",
    "cm2 = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(cm2)\n",
    "cm = cm / cm.astype(np.float).sum(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After removing all non-origination related information a logistic regression model is able to predict around 60% of paid in full vs \n",
    "charge-offs based only on origination data. While the overal rate is potentially acceptable, the model missed around \n",
    "25% of charge-offs and predicted a charge-off on a substantial amount of loans which are paid in full.\n",
    "\n",
    "In terms of predicting the 'grade' of the borrower however, a logistic regression model was only able to predict 25% of the 'grades\n",
    "of the loans. The initial model included interest rate and installement payment (whichare both a result of the grade) and\n",
    "thus had a higher predictive power, however those variable were removed. Two models to predict the grade were evaluated: 1)\n",
    "based on the full predcitor set and 2) based on the output from the charge-off model. They both performed about the same, with\n",
    "a 1 in 4 accuracy.\n",
    "\n",
    "The random forest model was able to predict grade better than a logistic regression model with about a 50% out of sample test. \n",
    "However the random forest model appears to over fit the data. For predicting default the random forest model failed to predict\n",
    "any defaults and had 100% of the data predict to perform. Overal, I would not recommend a random forest model for the data as it\n",
    "stands.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
