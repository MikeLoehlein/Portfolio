{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Introdution:\n",
    "# This Python code file estimates a seires of classifiation models with the goal of predicting whether an \n",
    "# employee will leave a company or not. The data is sourced from Kaggle: https://www.kaggle.com/ludobenistant/hr-analytics.\n",
    "# The code below has several sections: \n",
    "# - Load libraries and user defined functions\n",
    "# - Load the data and perform exploratory data analysis using univariate and bivariate charts and tables\n",
    "# - Prepare the data for modeling\n",
    "# - Estimate classifiers:\n",
    "#   - Logistic Regression\n",
    "#   - Support Vecotor Machine\n",
    "#   - Random Forest Decision Tree\n",
    "#   - Naive Bayes\n",
    "#   - Ensamble of Logistic Regression, Random Forest Decision Tree, and Naive Bayes\n",
    "# For each classifier, there are two sections: 1) hyper paramter grid search, and 2) model estimation with the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries and define functions\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "#User Defined Functions\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"Print a confusion matrix with user provided labels \"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()\n",
    "        \n",
    "def classification_output_report(X_train, y_train, X_test, y_test, y_pred, y_pred_prob, train_accuracy, test_accuracy):\n",
    "    '''For a given set of classification model inputs and outputs, print a standard set of outputs\n",
    "       including model accuracy, confusion matrix, classification report, and ROC Curve/AUC. \n",
    "       \n",
    "       Parameters:\n",
    "           - X_train: The model training data\n",
    "           - y_train: The model training labels\n",
    "           - X_test: The model testing data\n",
    "           - y_test: The model testing labels\n",
    "           - y_pred: The predicted labels, from model.predict()\n",
    "           - y_pred_prob: The predicted label probabilities, from model.predict_log_proba()\n",
    "           - train_accuracy: The accuracy of the model on the training data, from model.score(X_train, y_train)\n",
    "           - test_accuracy: The accuracy of the model on the testing data, from model.score(X_test, y_test)\n",
    "           \n",
    "       Note: If a model does not by default estimate a probability then y_pred_prob should be set to None'''\n",
    "    \n",
    "    #Print In and Out of Sample Accuracy\n",
    "    diff_accuracy = train_accuracy - test_accuracy\n",
    "\n",
    "    print('Accuracy of classifier on train set: {:.2f}'.format(train_accuracy))\n",
    "    print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(test_accuracy))\n",
    "    print('Difference in Accuracy - Train Minus Test: {:.8f}'.format(diff_accuracy))\n",
    "\n",
    "    #Confusion Matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print()\n",
    "    print(\"Confusion Matrix\")\n",
    "    print_cm(cf_matrix, [\"Stayed\", \"Left\"])\n",
    "\n",
    "    #Classification Report\n",
    "    print()\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    #ROC Curve + AUC\n",
    "    if y_pred_prob is None:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='AUC = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()        \n",
    "\n",
    "\n",
    "def grid_search(scores, model, param, name):\n",
    "    '''Perform a grid search over a set of hyper paramters for a model\n",
    "    \n",
    "       Parameters:\n",
    "           - scores: The list of metrics to be evaluated for, Options: accuracy, recall, precision, f1 \n",
    "           - model: The function name of the model\n",
    "           - param: A dictionary of parameters to search over\n",
    "           - name: A string with the name of the model to use in printing the report\n",
    "    '''\n",
    "    for score in scores:\n",
    "        print('Evaluating the ' + name + 'for optimal hyperparamters')\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        #Perform grid search using cross validation\n",
    "        clf = GridSearchCV(model(), param, cv=10, scoring='%s' % score)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "        \n",
    "#EDA Functions\n",
    "def numeric_eda_scripts(table, column):\n",
    "    '''\n",
    "    Numeric EDA script which plots a historgraph of the data and the mean, median, and 25th/75th percentiles.\n",
    "    \n",
    "    Parameters\n",
    "        - table: The table name\n",
    "        - column: The column name, formatted as a string\n",
    "    '''\n",
    "    print(\"Number of NA values: \" + str(table[column].isnull().sum()))\n",
    "    \n",
    "    plt.hist(table[column], bins=20, rwidth=.9)\n",
    "    plt.title(\"Histograph of \" + column.title() + \" - Total Records = \" + str(len(table)))\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Average \" + column.title() + \": \" + str(np.round(np.mean(table[column]),2)))\n",
    "    print(\"Median \" + column.title() + \": \" + str(np.median(table[column])))\n",
    "    print(\"25 Percentile \" + column.title() + \": \" + str(np.percentile(table[column], 25)))\n",
    "    print(\"75 Percentile \" + column.title() + \": \" + str(np.percentile(table[column], 75))) \n",
    "\n",
    "def cat_eda_scripts(table, column): \n",
    "    '''\n",
    "    Numeric EDA script which print the value counts and plots a historgraph of the value counts of the specified column.\n",
    "    \n",
    "    Parameters\n",
    "        - table: The table name\n",
    "        - column: The column name, formatted as a string\n",
    "    '''\n",
    "    data = table\n",
    "    data = data[column].apply(str)\n",
    "\n",
    "    data.value_counts().plot(kind='bar')\n",
    "    plt.title(\"Histograph of \" + column + \" - Total Records = \" + str(len(data)))\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    print(data.value_counts())\n",
    "    \n",
    "def numeric_by_cat_eda(table, col1, col2):\n",
    "    '''\n",
    "    Numeric EDA script which plots a grouped boxplot of the numeric data (col1) by the categorical data (col2).\n",
    "    \n",
    "    Parameters\n",
    "        - table: The table name\n",
    "        - col1: The numeric column name, formatted as a string\n",
    "        - col1: The cateogrical column name, formatted as a string\n",
    "    '''\n",
    "    data = table\n",
    "    data[col2] = data[col2].apply(str)\n",
    "\n",
    "    plt.title(\"Group Box Plot of \" + col1 + \" by \" + col2)\n",
    "    p = sns.boxplot(y=col1, x=col2, data=data)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def cat_by_cat_eda_scripts(table, col1, col2):\n",
    "    '''\n",
    "    Categorical EDA script which plots a grouped bar chart of the data (col1) by the data (col2). Two plots are \n",
    "    created, one of the value counts and one of the percentages within the by group.\n",
    "    \n",
    "    Parameters\n",
    "        - table: The table name\n",
    "        - col1: The 1st categorical column name, formatted as a string\n",
    "        - col1: The 2nd cateogrical column name, formatted as a string\n",
    "    '''\n",
    "    \n",
    "    #Plot value counts by group\n",
    "    plt.axes([0.05,0.05,0.425,0.9])\n",
    "    plt.title('Counts by Group - ' + col2)\n",
    "    sns.countplot(x=col1, orient='v', hue=col2, data=table, palette=\"Greens_d\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    #Plot percentages by group\n",
    "    plt.axes([.8,0.05,0.425,0.9])\n",
    "    rates = (table.groupby([col2])[col1]\n",
    "                         .value_counts(normalize=True)\n",
    "                         .rename('percentage')\n",
    "                         .mul(100)\n",
    "                         .reset_index()\n",
    "                         .sort_values(col1))\n",
    "\n",
    "    plt.title('Percentages by Group - ' + col2)\n",
    "    p = sns.barplot(x=col1, y=\"percentage\", hue=col2, data=rates, palette=\"Greens_d\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "hr_data = pd.read_csv(\"C:/Personal/Kaggle/human-resources-analytics/Data/HR_comma_sep.csv\") \n",
    "#Source: https://www.kaggle.com/ludobenistant/hr-analytics\n",
    "\n",
    "print(\"Number of Rows: \" + str(len(hr_data))) #Check Row Count\n",
    "\n",
    "print()\n",
    "print(\"First Five Rows:\")\n",
    "print(hr_data.iloc[0:5]) #Check First 5 Rows\n",
    "\n",
    "print()\n",
    "print(\"Final 5 Rows:\")\n",
    "print(hr_data.iloc[-5:]) #Check Final 5 Rows\n",
    "\n",
    "print()\n",
    "print(\"Column Data Types\")\n",
    "print(hr_data.dtypes) #Print the colum types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Satisfaction Level\n",
    "numeric_eda_scripts(hr_data, \"satisfaction_level\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, satisfaction_level is defined \n",
    "      as: \"Level of satisfaction (0-1)\".\n",
    "      \n",
    "      There are no missing/NA values within this column. In terms of \n",
    "      data quality, all of the data points are between 0 and 1 \n",
    "      which gives reasonable assurance that the data has no outliers.\n",
    "\n",
    "      Satisfaction level appears to follow a bimodal distribution with a \n",
    "      spike in the number employees having very low levels of satisfication (<0.2) and \n",
    "      the majority of employees having higher levels of satisfacation (>0.4 \n",
    "      or >0.5). The 25th percentile and 75th percentile are about the same \n",
    "      distance away from the median (distance of 0.18 to 0.20) which implies \n",
    "      the body of the distribution is symetrical.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Last Evaluation\n",
    "numeric_eda_scripts(hr_data, \"last_evaluation\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, satisfaction_level is defined \n",
    "      as: \"Time since last performance evaluation (in Years)\".\n",
    "      \n",
    "      There are no missing/NA values within this column. In terms of \n",
    "      data quality, all of the data points are between 0 and 1. This means\n",
    "      that all employees have had an evaluation in the past year. This implies\n",
    "      that there are not outliers in the data.\n",
    "\n",
    "      The data is multi modal with spikes in the data at 1, about .5 and at about .85.\n",
    "      The 25th percentile and 75th percentile are about the same \n",
    "      distance away from the median (distance of 0.16) which implies \n",
    "      the body of the distribution is symetrical. \n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Average Monthly Hours\n",
    "numeric_eda_scripts(hr_data, \"average_montly_hours\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, average_montly_hours is defined \n",
    "      as: \"Average monthly hours at workplace\".\n",
    "      \n",
    "      There are no missing/NA values within this column. The prior assumption \n",
    "      that a standard work week is 40 hours per week means that there is on \n",
    "      average 160 or so hours in a month. If a worker is doing more than \n",
    "      160 than they are working overtime. The data shows that no employee is \n",
    "      below about 100 hours per month. The mode is at about 160. However the data is \n",
    "      shows a lot of values above 160. The max values appear at about 300 hours, which \n",
    "      imply a 75 hour work week, which is plausible but very high.\n",
    "\n",
    "      The 25th percentile and 75th percentile are about the same \n",
    "      distance away from the median (distance of about 45) which implies \n",
    "      the body of the distribution is symetrical. \n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Promotion in the Last 5 Years\n",
    "cat_eda_scripts(hr_data, \"promotion_last_5years\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, promotion_last_5years is defined \n",
    "      as: \"Whether the employee was promoted in the last five years\".\n",
    "      \n",
    "      Most employee's did not get a promotion.       \n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Left\n",
    "cat_eda_scripts(hr_data, \"left\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, left is defined \n",
    "      as: \"Whether the employee left the workplace or not (1 or 0)\".\n",
    "      \n",
    "      Most employee's did not leave though about 23% did.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Number of Projects\n",
    "numeric_eda_scripts(hr_data, \"number_project\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, number_project is defined \n",
    "      as: \"Number of projects completed while at work\".\n",
    "    \n",
    "      There are no missing values in this column. Range of values\n",
    "      is between 2 and 7.\n",
    "      \n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Time Spent with the Company\n",
    "numeric_eda_scripts(hr_data, \"time_spend_company\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, time_spend_company is defined \n",
    "      as: \"Number of years spent in the company\".\n",
    "      \n",
    "      From the histogram it looks like the employees at the company \n",
    "      all have been there at least 1 year though most are either 2 or 3 year tenured. \n",
    "      This could imply that after 3 years that there is a natural churn at the company.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Work Accident\n",
    "cat_eda_scripts(hr_data, \"Work_accident\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, Work_accident is defined \n",
    "      as: \"Whether the employee had a workplace accident\".\n",
    "       \n",
    "      2000 (15%) of employees had a work accident. This seems like a \n",
    "      high rate.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Sales\n",
    "cat_eda_scripts(hr_data, \"sales\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, sales is defined \n",
    "      as: \"Department in which they work for\".\n",
    "       \n",
    "      The largest number of employees are in sales and techincal departments.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Salary\n",
    "cat_eda_scripts(hr_data, \"salary\")\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Based on the metadata provided, salary is defined \n",
    "      as: \"Relative level of salary (high)\".\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Number of Projects by Left\n",
    "cat_by_cat_eda_scripts(hr_data, 'number_project', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      When grouping the number_projects by left, it appears that over 40% of the \n",
    "      people who left only completed 2 projects, while the rest of the people who left\n",
    "      completed 4 or more projects. Compared to the people who did not leave\n",
    "      most of the people completed 3 or 4 projects.\n",
    "      ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Sales by Left\n",
    "cat_by_cat_eda_scripts(hr_data, 'sales', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      The relative distribution of the people who left compared to the \n",
    "      department they are in appears to be fairly evenly distributed \n",
    "      when comparing those who left with those who did not.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Salary by Left\n",
    "cat_by_cat_eda_scripts(hr_data, 'salary', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      The employees who left had a higher percent of 'low salary' than those who \n",
    "      stayed and a lower percent of 'high salary' than those who stayed.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Promotion last 5 Years by Left\n",
    "cat_by_cat_eda_scripts(hr_data, 'promotion_last_5years', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Those who had a promotion in the last five year are slightly less likely to have left the company.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Work Accident by Left\n",
    "cat_by_cat_eda_scripts(hr_data, 'Work_accident', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      Those who have not had a work accident are more likely to have left the company.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Satisfaction Level by Left\n",
    "numeric_by_cat_eda(hr_data, 'satisfaction_level', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      The satisfation level of employees who stayed were typically between .55 and .85 (about) while those\n",
    "      who left were below .70. Overal, those who left the company had a lower satisfaction level than those \n",
    "      who stayed.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Last Evaluation by Left\n",
    "numeric_by_cat_eda(hr_data, 'last_evaluation', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      \n",
    "      Those who left had, on average an evaluation further in the past than those who stayed though \n",
    "      the body of the distribution is larger also.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for Average Monthly Hours by Left\n",
    "numeric_by_cat_eda(hr_data, 'average_montly_hours', 'left')\n",
    "\n",
    "print('''\n",
    "      Analyst Comments/Observations:\n",
    "      \n",
    "      The employees wholeft had on average a higher number of hours per month than those who stayed.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep for Modeling\n",
    "\n",
    "#Binarize the Class Variables\n",
    "X = hr_data.drop('left',axis=1)\n",
    "y = hr_data['left']\n",
    "y = y.astype(int)\n",
    "print(y.dtypes)\n",
    "\n",
    "print(X.dtypes)\n",
    "X = pd.get_dummies(X, prefix=['sales', 'salary'])\n",
    "print()\n",
    "print(X.head())\n",
    "\n",
    "print()\n",
    "print(X.dtypes)\n",
    "\n",
    "#Create Train and Test Data Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print()\n",
    "print(\"length of x = \" + str(len(X)))\n",
    "print(\"length of x train = \" + str(len(X_train)))\n",
    "print(\"length of x test = \" + str(len(X_test)))\n",
    "\n",
    "#Setup Kfold\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "print(\"Logistic Regression\")\n",
    "print()\n",
    "\n",
    "# Tunning the hyper paramters\n",
    "tuned_parameters = [{'penalty': ['l1','l2'], \n",
    "                     'C': [1, 10, 100, 1000],\n",
    "                     'class_weight': ['balanced',None]}]\n",
    "\n",
    "scores = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "grid_search(scores, LogisticRegression, tuned_parameters, \"Logistic Regression Model\")\n",
    "\n",
    "print('''\n",
    "    Analyst comments:\n",
    "    The final hyper parameters where selected basaed on the recall parameters and are:\n",
    "    {'class_weight': 'balanced', 'C': 1, 'penalty': 'l1'}\n",
    "    \n",
    "    The recall rate was 0.81  with these parameters.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "print(\"Logistic Regression - Selected Hyper Parameters\")\n",
    "print()\n",
    "\n",
    "#Fit the select hyper paramters\n",
    "logreg = LogisticRegression(penalty='l1', C=1, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = logreg.score(X_train, y_train)\n",
    "test_accuracy = logreg.score(X_test, y_test)\n",
    "\n",
    "#Calculate Out of Sample Predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n",
    " \n",
    "#K Fold Validation \n",
    "results = cross_val_score(logreg, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))    \n",
    "    \n",
    "#Model Train    \n",
    "classification_output_report(X_train, y_train, X_test, y_test, y_pred, y_pred_prob, train_accuracy, test_accuracy)\n",
    "\n",
    "print('''\n",
    "    Analyst Comments:\n",
    "    The Kfold accuracy is .75 while the train and test accuracy's were both 0.76.\n",
    "    The recall from the model is 0.81.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "print(\"Support Vector Machine - Hyper Paramter Selection\")\n",
    "print()\n",
    "\n",
    "# Tunning the hyper paramters\n",
    "tuned_parameters = [{'dual':[False],\n",
    "                     'penalty': ['l1','l2'], \n",
    "                     'C': [1, 10, 100, 1000],\n",
    "                     'class_weight': ['balanced',None]}]\n",
    "\n",
    "scores = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "grid_search(scores, LinearSVC, tuned_parameters, \"Linear SVM\")\n",
    "\n",
    "print('''\n",
    "    Analyst comments:\n",
    "    The final hyper parameters where selected basaed on the recall parameters and are:\n",
    "    {'class_weight': 'balanced', 'C': 1, 'dual': False, 'penalty': 'l1'}\n",
    "    \n",
    "    The recall rate was 0.80  with these parameters.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "print(\"Support Vector Machine - Selected Hyper Paramters\")\n",
    "print()\n",
    "\n",
    "#Fit The Model\n",
    "svm = LinearSVC(penalty='l1', dual=False, C=.5, class_weight='balanced')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = svm.score(X_train, y_train)\n",
    "test_accuracy = svm.score(X_test, y_test)\n",
    "\n",
    "#Calculate Out of Sample Predictions\n",
    "y_pred = svm.predict(X_test)\n",
    "y_pred_prob = None #svm.predict_proba(X_test) does not exist\n",
    "\n",
    "#K Fold Validation \n",
    "results = cross_val_score(svm, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))\n",
    "    \n",
    "#Model Train    \n",
    "classification_output_report(X_train, y_train, X_test, y_test, y_pred, y_pred_prob, train_accuracy, test_accuracy)\n",
    "\n",
    "print('''\n",
    "    Analyst Comments:\n",
    "    The Kfold accuracy is .75 while the train and test accuracy's were 76 and 75 respectively.\n",
    "    The recall from the model is 0.80.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "print(\"Random Forest - Hyper Paramter Selection\")\n",
    "print()\n",
    "\n",
    "# Tunning the hyper paramters\n",
    "tuned_parameters = [{'n_estimators':[250,500],\n",
    "                     'max_features': ['sqrt','log2'], \n",
    "                     'criterion': ['gini'],\n",
    "                     'max_depth': [25,50,None],\n",
    "                     'min_samples_split': [5,10],\n",
    "                     'min_samples_leaf': [5,10],\n",
    "                     'max_leaf_nodes': [None]}]\n",
    "\n",
    "scores = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "grid_search(scores, RandomForestClassifier, tuned_parameters, \"Random Forest\")\n",
    "\n",
    "print('''\n",
    "    Analyst comments:\n",
    "    The final hyper parameters where selected basaed on the recall parameters and are:\n",
    "    {'n_estimators': 250, 'max_depth': None, 'max_features': 'sqrt', 'criterion': 'gini', \n",
    "    'min_samples_split': 10, 'max_leaf_nodes': None, 'min_samples_leaf': 5}\n",
    "    \n",
    "    The recall rate was 0.91  with these parameters.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "print(\"Random Forest - Selected Hyper Paramters\")\n",
    "print()\n",
    "\n",
    "#Fit The Model\n",
    "rf_clf = RandomForestClassifier(oob_score=True, n_jobs=2, n_estimators=250, \n",
    "                                        max_features='sqrt', criterion='gini', max_depth=None, \n",
    "                                        min_samples_split=10, min_samples_leaf=5, max_leaf_nodes=None)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = rf_clf.score(X_train, y_train)\n",
    "test_accuracy = rf_clf.score(X_test, y_test)\n",
    "\n",
    "#Calculate Out of Sample Predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "y_pred_prob = rf_clf.predict_proba(X_test)\n",
    "    \n",
    "#K Fold Validation \n",
    "results = cross_val_score(rf_clf, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))    \n",
    "    \n",
    "#Model Train    \n",
    "classification_output_report(X_train, y_train, X_test, y_test, y_pred, y_pred_prob, train_accuracy, test_accuracy)\n",
    "\n",
    "print('''\n",
    "    Analyst Comments:\n",
    "    The Kfold accuracy is .97 while the train and test accuracy's were 98 and 97 respectively.\n",
    "    The recall from the model is 0.91.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "#Fit The Model\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print()\n",
    "\n",
    "train_accuracy = nb_clf.score(X_train, y_train)\n",
    "test_accuracy = nb_clf.score(X_test, y_test)\n",
    "\n",
    "#Calculate Out of Sample Predictions\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "y_pred_prob = nb_clf.predict_proba(X_test)\n",
    "\n",
    "#K Fold Validation \n",
    "results = cross_val_score(nb_clf, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))    \n",
    "    \n",
    "#Model Train    \n",
    "classification_output_report(X_train, y_train, X_test, y_test, y_pred, y_pred_prob, train_accuracy, test_accuracy)\n",
    "\n",
    "print('''\n",
    "    Analyst Comments:\n",
    "    The Kfold accuracy is .63 while the train and test accuracy's were 0.66 and 0.67 respectively.\n",
    "    The recall from the model is 0.84.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensamble Model of Top Three\n",
    "#Fit The Model\n",
    "logreg = LogisticRegression(penalty='l1', C=1, class_weight='balanced')\n",
    "gbt_clf = RandomForestClassifier(oob_score=True, n_jobs=2, n_estimators=250, \n",
    "                                        max_features='sqrt', criterion='gini', max_depth=None, \n",
    "                                        min_samples_split=10, min_samples_leaf=5, max_leaf_nodes=None)\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "ensamble = VotingClassifier(estimators=[('lr', logreg), ('gbt', gbt_clf), (\"nb\", nb_clf)], \n",
    "                         voting='hard')\n",
    "\n",
    "ensamble.fit(X_train, y_train)\n",
    "print(\"Ensamble Model\")\n",
    "print()\n",
    "\n",
    "train_accuracy = ensamble.score(X_train, y_train)\n",
    "test_accuracy = ensamble.score(X_test, y_test)\n",
    "\n",
    "#Calculate Out of Sample Predictions\n",
    "y_pred = ensamble.predict(X_test)\n",
    "y_pred_prob = None #ensamble.predict_proba(X_test)\n",
    "\n",
    "#K Fold Validation \n",
    "results = cross_val_score(ensamble, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))    \n",
    "    \n",
    "#Model Train     \n",
    "classification_output_report(X_train, y_train, X_test, y_test, y_pred, y_pred_prob, train_accuracy, test_accuracy)\n",
    "\n",
    "print('''\n",
    "    Analyst Comments:\n",
    "    The Kfold accuracy is .82 while the train and test accuracy's were 0.83 and 0.84 respectively.\n",
    "    The recall from the model is 0.88.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\n",
    "Summary and Model Recommendation:\n",
    "\n",
    "    Based on the above analysis the data provided is judged to be acceptable and ready for modeling.\n",
    "    \n",
    "    Of the modeling algorithms tested (logistic regresion, SVM, random forest, naive bayes, ensamble of the above) the\n",
    "    best model, based on the recall metric, is the random forest. The second best model is the ensamble model.\n",
    "    \n",
    "    Recall was selected as the primary metric of interest because false positive prediction (predicting someone who\n",
    "    would leave would not) is a lower cost item than false negative (predicting someone as not leaving who does leave) \n",
    "    for the company. The cost to HR of doing an employee intervention is less than that of hiring and training a new employee.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
