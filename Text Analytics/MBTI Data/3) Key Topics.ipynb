{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'3) Key Topic Modeling'\n",
    "\n",
    "'''\n",
    "This file takes the data that was prepared in the 1) Data Prep and uses it to extract the top 20 \n",
    "key topics, by MBTI type, for the comments. \n",
    "\n",
    "The files does the following:\n",
    "    - Loads libraries and User Defined Functions\n",
    "    - Loads the Data and Creates a list of MBTI types\n",
    "    - Loops over the comments data by MBTI and extracts the top 20 topics and prints them to the screen\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from gensim import corpora, models\n",
    "from normalization import normalize_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load User Defined Functions\n",
    "def print_topics_gensim(topic_model, total_topics=1,\n",
    "                        weight_threshold=0.0001,\n",
    "                        display_weights=False,\n",
    "                        num_terms=None):\n",
    "    \n",
    "    for index in range(total_topics):\n",
    "        topic = topic_model.show_topic(index)\n",
    "        topic = [(word, round(wt,2)) \n",
    "                 for word, wt in topic \n",
    "                 if abs(wt) >= weight_threshold]\n",
    "        if display_weights:\n",
    "            print('Topic #'+str(index+1)+' with weights')\n",
    "            print(topic[:num_terms] if num_terms else topic)\n",
    "        else:\n",
    "            print('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms] if num_terms else tw)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned MBTI data\n",
    "cleaned_mbti_token_userlvl = pd.read_pickle(\"cleaned_mbti_token_userlvl.pkl\")\n",
    "print(cleaned_mbti_token_userlvl.head())\n",
    "\n",
    "mbti_list = cleaned_mbti_token_userlvl.iloc[:,0].values.tolist()\n",
    "mbti_list = list(set(mbti_list))\n",
    "\n",
    "print(mbti_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each MBTI Type in the data, extract the top 20 topics \n",
    "\n",
    "total_topics = 20 #Number of topics\n",
    "\n",
    "#Define the words to be removed from the lists\n",
    "wordlist = ['url','infj','intj','infp','intp','enfj','entj','enfp','entp','isfj',\n",
    "                'istj','isfp','istp','esfj','estj','esfp','estp','tapatalk']\n",
    "\n",
    "for mbti in mbti_list:\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print(mbti)\n",
    "    \n",
    "    #Subset the data\n",
    "    subset = cleaned_mbti_token_userlvl.loc[cleaned_mbti_token_userlvl['type'] == mbti]\n",
    "    \n",
    "    #Transform the data to list form\n",
    "    features = subset.iloc[:,4:].values.tolist()\n",
    "\n",
    "    #Remove common/superfuerlous words from the lists\n",
    "    feature_none = []\n",
    "    for x in features:\n",
    "        y = list(filter(None.__ne__, x))\n",
    "        z = [z for z in y if z not in wordlist]    \n",
    "        feature_none.append(z)\n",
    "\n",
    "    labels = cleaned_mbti_token_userlvl.iloc[:,[1,3]].values.tolist()\n",
    "\n",
    "    #Create a dictionary of the words\n",
    "    dictionary = corpora.Dictionary(feature_none)\n",
    "    #print( dictionary.token2id)\n",
    "\n",
    "    #Transform the document to a BOW\n",
    "    corpus = [dictionary.doc2bow(text) for text in feature_none]\n",
    "    #print(corpus[:2])\n",
    "\n",
    "    #Transform to TFIDF\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "    #Extract top topics using Latent Semantic Indexing\n",
    "    lsi = models.LsiModel(corpus_tfidf, \n",
    "                          id2word=dictionary, \n",
    "                          num_topics=total_topics)\n",
    "\n",
    "    #Print the top topics\n",
    "    print_topics_gensim(topic_model=lsi,\n",
    "                        total_topics=total_topics,\n",
    "                        num_terms=15,\n",
    "                        display_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Observations of the topics by MBTI type:\n",
    "- Across the MBTI types some common theme occur:\n",
    "    Personality\n",
    "    Relationship\n",
    "    Music\n",
    "    \n",
    "- When reviewing across the MBTI types, many of the key topics are difficult to summaries. This is likely due to the wide \n",
    "variety of topics discussed on the target forum. When summarizing at the user level the various topics appear to be intermingled.\n",
    "Reviewing the key topics using comment level data may provide more insights. \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
