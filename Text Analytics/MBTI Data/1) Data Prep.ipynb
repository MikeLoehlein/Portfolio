{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'1) Data Prep'\n",
    "\n",
    "'''\n",
    "This file prepares the MBTI source data. It does the following:\n",
    "    - Imports the MBTI data sourced from: https://www.kaggle.com/datasnaek/mbti-type\n",
    "    - Performs a basic spell check on the data\n",
    "    - Normalizes the data\n",
    "        - Normalizes without tokenization\n",
    "        - Normalizes with tokenization\n",
    "    - Performs SVD on the data to reduce the features\n",
    "    - Saves the cleaned data a set of pickle files for modeling\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Import libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "#Import Functions from Libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import user defined functions\n",
    "from feature_extractors import *\n",
    "from normalization import *\n",
    "from spelling_corrector import *\n",
    "from contractions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initalize Stop Words and Lemmatizer\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Load the data\n",
    "mbti = pd.read_csv(\"C:/Personal/Kaggle/mbti-myers-briggs-personality-type-dataset/mbti_1.csv\") \n",
    "#Source: https://www.kaggle.com/datasnaek/mbti-type\n",
    "\n",
    "#Print top five rows\n",
    "print(mbti.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot MBTI Type Distrbution\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.xticks(fontsize=24, rotation=0)\n",
    "plt.yticks(fontsize=24, rotation=0)\n",
    "sns.countplot(data=mbti, x='type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Clean The Comments\n",
    "df = mbti\n",
    "\n",
    "#Convert MBIT to Class\n",
    "unique_type_list = df.type.unique()\n",
    "lab_encoder = LabelEncoder().fit(unique_type_list)\n",
    "\n",
    "with open(\"LabelEncoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lab_encoder, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "df['type_enc'] = lab_encoder.transform(df['type'])\n",
    "\n",
    "#Replace Links\n",
    "df['comment'] = df.posts.str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','url')\n",
    "\n",
    "#Split comments based in pipes, replace with white space\n",
    "df['comment'] = df.comment.str.replace('\\|+',' ')\n",
    "\n",
    "#Keep Only ASCII\n",
    "df['comment'] = df.comment.str.replace('[^\\x00-\\x7F]+','')\n",
    "\n",
    "#Remove Selected Puncation\n",
    "df['comment'] = df.comment.str.replace('[,.:!@#$%&*()_+?><]+',' ')\n",
    "\n",
    "#Remove Words Longer than 20 characters\n",
    "df['comment'] = df.comment.str.replace('[a-zA-Z0-9_]{20,}',' ')                                       \n",
    "                                       \n",
    "#Remove Extra White Space\n",
    "df['comment'] = df.comment.str.replace('[\\s]+',' ')                                       \n",
    "                                                                              \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Spelling\n",
    "#df = df.sample(n=100) #Sample For Testing\n",
    "\n",
    "#Correct Spelling\n",
    "comments = df['comment'].tolist()\n",
    "print(len(comments))\n",
    "print(type(comments))\n",
    "print()\n",
    "\n",
    "dta = []\n",
    "i = 0\n",
    "for x in comments:\n",
    "    if (i % 100 == 0):\n",
    "        print(str(datetime.datetime.now()))\n",
    "        print(\"Record \" + str(i) + \" of \" + str(len(comments)))\n",
    "    x = ' '.join([correct_text_generic(y) for y in x.split(' ')])\n",
    "    dta.append(x)\n",
    "    i += 1\n",
    "                 \n",
    "print(dta[0:2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Normalize Data\n",
    "#No Tokenization\n",
    "dta_notoken = normalize_corpus(dta, tokenize=False, contraction=CONTRACTION_MAP)\n",
    "print(dta_notoken[0:2])\n",
    "print()\n",
    "\n",
    "clean = pd.DataFrame({'clean_comment':dta_notoken})\n",
    "cleaned = pd.concat([df, clean], axis=1)\n",
    "print(cleaned[0:2])\n",
    "print()\n",
    "\n",
    "cleaned.to_pickle(\"cleaned_mbti_userlvl.pkl\")\n",
    "\n",
    "####################################################################3\n",
    "#Tokenization\n",
    "dta_token = normalize_corpus(dta, tokenize=True, contraction=CONTRACTION_MAP)\n",
    "print(dta_token[0:2])\n",
    "print()\n",
    "\n",
    "clean = pd.DataFrame(dta_token)\n",
    "cleaned = pd.concat([df, clean], axis=1)\n",
    "print(cleaned[0:2])\n",
    "print()\n",
    "\n",
    "cleaned.to_pickle(\"cleaned_mbti_token_userlvl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "X = dta_notoken\n",
    "y = df['type_enc']\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "\n",
    "print(len(X_train))\n",
    "print(X_train[:2])\n",
    "print()\n",
    "\n",
    "print(len(X_test))\n",
    "print(X_test[:2])\n",
    "print()\n",
    "\n",
    "print(len(y_train))\n",
    "print(y_train[:2])\n",
    "print()\n",
    "\n",
    "print(len(y_test))\n",
    "print(y_test[:2])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "bow_vectorizor, bow_features = bow_extractor(X_train, ngram_range=(1,1))\n",
    "bow_features = bow_features.todense()\n",
    "print(\"bow_features shape: \" + str(bow_features.shape))\n",
    "\n",
    "tfidf_vectorizor, tfidf_features = tfidf_transformer(bow_features)\n",
    "tfidf_features = tfidf_features.todense()\n",
    "print(\"tfidf_features shape: \" + str(tfidf_features.shape))\n",
    "\n",
    "bow_features_test = bow_vectorizor.transform(X_test)\n",
    "bow_features_test = bow_features_test.todense()\n",
    "print(\"bow_features_test shape: \" + str(bow_features_test.shape))\n",
    "\n",
    "tfidf_features_test = tfidf_vectorizor.transform(bow_features_test)\n",
    "tfidf_features_test = tfidf_features_test.todense()\n",
    "print(\"tfidf_features_test shape: \" + str(tfidf_features_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save Processed Data to Disk\n",
    "clean_bow = pd.DataFrame(bow_features)\n",
    "cleaned_bow = pd.concat([y_train.reset_index(), clean_bow], axis=1)\n",
    "cleaned_bow.to_pickle(\"cleaned_bow_train_userlvl.pkl\")\n",
    "\n",
    "clean_tfidf = pd.DataFrame(tfidf_features)\n",
    "cleaned_tfidf = pd.concat([y_train.reset_index(), clean_tfidf], axis=1)\n",
    "cleaned_tfidf.to_pickle(\"cleaned_tfidf_train_userlvl.pkl\")\n",
    "\n",
    "clean_bow = pd.DataFrame(bow_features_test)\n",
    "cleaned_bow = pd.concat([y_test.reset_index(), clean_bow], axis=1)\n",
    "cleaned_bow.to_pickle(\"cleaned_bow_test_userlvl.pkl\")\n",
    "\n",
    "clean_tfidf = pd.DataFrame(tfidf_features_test)\n",
    "cleaned_tfidf = pd.concat([y_test.reset_index(), clean_tfidf], axis=1)\n",
    "cleaned_tfidf.to_pickle(\"cleaned_tfidf_test_userlvl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVD\n",
    "\n",
    "#BOW SVD\n",
    "svd = TruncatedSVD(n_components=150)\n",
    "svd.fit(bow_features)\n",
    "bow_features_svd = svd.transform(bow_features)\n",
    "bow_features_svd = pd.DataFrame(bow_features_svd)\n",
    "bow_features_svd = pd.concat([y_train.reset_index(), bow_features_svd], axis=1)\n",
    "bow_features_svd.to_pickle(\"cleaned_bow_train_userlvl_svd.pkl\")\n",
    "\n",
    "bow_features_test_svd = svd.transform(bow_features_test)\n",
    "bow_features_test_svd = pd.DataFrame(bow_features_test_svd)\n",
    "bow_features_test_svd = pd.concat([y_test.reset_index(), bow_features_test_svd], axis=1)\n",
    "bow_features_test_svd.to_pickle(\"cleaned_bow_test_userlvl_svd.pkl\")\n",
    "\n",
    "#TFIDF SVD\n",
    "svd = TruncatedSVD(n_components=150)\n",
    "svd.fit(tfidf_features)\n",
    "tfidf_features_svd = svd.transform(tfidf_features)\n",
    "tfidf_features_svd = pd.DataFrame(bow_features_test_svd)\n",
    "tfidf_features_svd = pd.concat([y_train.reset_index(), tfidf_features_svd], axis=1)\n",
    "tfidf_features_svd.to_pickle(\"cleaned_tfidf_train_userlvl_svd.pkl\")\n",
    "\n",
    "tfidf_features_test_svd = svd.transform(tfidf_features_test)\n",
    "tfidf_features_test_svd = pd.DataFrame(tfidf_features_test_svd)\n",
    "tfidf_features_test_svd = pd.concat([y_test.reset_index(), tfidf_features_test_svd], axis=1)\n",
    "tfidf_features_test_svd.to_pickle(\"cleaned_tfidf_test_userlvl_svd.pkl\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
