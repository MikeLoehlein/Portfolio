{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Introduction\n",
    "\n",
    "This Python script scrapes the beer advocate website and downloads selected information for the beers listed on the first pages \n",
    "(current 4) of each style. Each style page has about 50 links, for an estimated 200 (max) links per beer style. \n",
    "\n",
    "For each of the beers scraped, it saves the picture of the beer, the beer ratings, beer information, and names/company which \n",
    "produces the beer. The script then produces high level summaries of the information which it scrapes. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from urllib.request import Request, urlopen, urlretrieve, URLopener\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#User Defined Functions:\n",
    "\n",
    "#Get Style Links\n",
    "def Beer_Style_Links():\n",
    "    '''\n",
    "    This function scrapes all of the 'beer style' links from Beer Advocate's style page. It then saves those \n",
    "    links to a list which is returned. \n",
    "    \n",
    "    This function takes no parameters.\n",
    "    '''\n",
    "    try:\n",
    "        #Open the site and load the BeautifulSoup object\n",
    "        site= \"https://www.beeradvocate.com/beer/style/\"\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "        req = Request(site,headers=hdr)\n",
    "        html = urlopen(req)\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "    except Exception as e:\n",
    "        #Save any errors to the log\n",
    "        logger.error(str(e))\n",
    "        logger.error(str(site))\n",
    "    else:\n",
    "        #Process the BeautifulSoup object and create a list of beer sylte links\n",
    "        names = bsObj.find(\"div\", {\"id\":\"ba-content\"}).findAll(\"a\", href=re.compile(\"^(/beer/style/)((?!:).)*$\"))\n",
    "        links = []\n",
    "\n",
    "        for i in range(len(names)-1):\n",
    "            links.append(names[i].attrs[\"href\"])\n",
    "\n",
    "        return links\n",
    "    \n",
    "#Get Beer Links\n",
    "def Beer_Links(link, depth, beer_links):\n",
    "    '''\n",
    "    This function takes a beer style link and saves all of the beer profile links from the page to a list. it is set\n",
    "    so that it will go to the 'next page' of the beers for that style up to the 'depth' number of pages. \n",
    "    If depth = 3 then it will scrape the first three pages of the beer style. The function returns a list\n",
    "    of beer links.\n",
    "    \n",
    "    Parameters:\n",
    "        link = is the beer style link which should be scraped\n",
    "        depth = is the number of pages the fucntion should scrape for beer links\n",
    "        beer_links = is the list which function should append the scrapped beer links to.\n",
    "    '''\n",
    "    page = link\n",
    "    j = 0\n",
    "    \n",
    "    while j < depth:\n",
    "        try:\n",
    "            #Open the site and load the BeautifulSoup object\n",
    "            site= \"https://www.beeradvocate.com\" + str(page)\n",
    "            hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "            req = Request(site,headers=hdr)\n",
    "            html = urlopen(req)\n",
    "            bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "        except Exception as e:\n",
    "            #Save any errors to the log\n",
    "            logger.error(str(e))\n",
    "            logger.error(str(site))\n",
    "        else:\n",
    "            #Process the BeautifulSoup object and create append\n",
    "            # the beer links to the list\n",
    "            names = bsObj.find(\"div\", {\"id\":\"ba-content\"}).findAll(\"a\", href=re.compile(\"^(/beer/profile/[0-9]+/[0-9]+)((?!:).)*$\"))\n",
    "\n",
    "            for i in range(len(names)-1):\n",
    "                beer_links.append(names[i].attrs[\"href\"])\n",
    "\n",
    "        #get the next page link(s) from the page\n",
    "        next_link = bsObj.find(\"div\", {\"id\":\"ba-content\"}).findAll(\"a\")\n",
    "\n",
    "        next_page = []\n",
    "        for h in range(len(next_link)-1):\n",
    "            if next_link[h].getText() == 'next':\n",
    "                next_page.append(next_link[h])\n",
    "\n",
    "        #if there are multiple links with the text next, take the first one.\n",
    "        if len(next_page) > 1:\n",
    "            next_page = next_page[0].attrs[\"href\"]\n",
    "        page = next_page\n",
    "        j += 1\n",
    "\n",
    "#Get Beer Information\n",
    "def Beer_Info(page, writer):\n",
    "    '''\n",
    "    This function scrapes the selected information for a specific beer profile and saves it to a .csv file. It also\n",
    "    saves the image to a BeerImages folder which the user needs to create prior to running the code. \n",
    "    \n",
    "    The function scrapes the:\n",
    "        - beer name\n",
    "        - beer stats\n",
    "        - beer information\n",
    "        - beer score\n",
    "    sections of the beer profile.\n",
    "    \n",
    "    Parameters\n",
    "        - page = the beer page to be scraped\n",
    "        - writer = the writer object which the output should be saved to.\n",
    "    '''\n",
    "    try:\n",
    "        #Open the site and load the BeautifulSoup object\n",
    "        site= \"https://www.beeradvocate.com\" + page\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "        req = Request(site,headers=hdr)\n",
    "        html = urlopen(req)\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "    except Exception as e:\n",
    "        #Save any errors to the log\n",
    "        logger.error(str(e))\n",
    "        logger.error(str(site))\n",
    "    else:\n",
    "        #Process the BeautifulSoup object\n",
    "        try:\n",
    "            #Save the image to disk\n",
    "            image = bsObj.find(\"div\", {\"id\":\"info_box\"}).find('img')['src']\n",
    "            name = bsObj.find(\"div\", {\"id\":\"info_box\"}).find('img')['alt']\n",
    "            name = re.sub(r'[^a-zA-Z0-9]+','', name)\n",
    "\n",
    "            req = Request(image,headers=hdr)\n",
    "            resource = urlopen(req)\n",
    "            output = open(\"BeerImages/\" + name +\".jpg\",\"wb\")\n",
    "            output.write(resource.read())\n",
    "            output.close() \n",
    "        except Exception as e:\n",
    "            #Save any errors to the log\n",
    "            logger.error(str(e))\n",
    "            logger.error(str(image))\n",
    "\n",
    "        #Scrape the selecte data from the webpage\n",
    "        item = {}\n",
    "        item['name'] = re.sub('\\n|\\t', '||', bsObj.find(\"h1\").getText())\n",
    "        item['score'] = re.sub('\\n|\\t', '||', bsObj.find(\"div\", {\"id\":\"score_box\"}).getText())  \n",
    "        item['stats'] = re.sub('\\n|\\t', '||', bsObj.find(\"div\", {\"id\":\"stats_box\"}).getText())  \n",
    "        item['info'] =  re.sub('\\n|\\t', '||', bsObj.find(\"div\", {\"id\":\"info_box\"}).getText())\n",
    "        dta = pd.Series(item, name='item')\n",
    "        dta = dta.str.encode('utf-8') \n",
    "        \n",
    "        writer.writerow(dta)\n",
    "\n",
    "#Error Logging\n",
    "def error_log():\n",
    "    '''\n",
    "    This function creates an error logger which is used to track the exceptions caught by the function. \n",
    "    It saves a text file in the folder which contains the source code.\n",
    "    '''\n",
    "    global logger\n",
    "    logger = logging.getLogger(\"File Log\")\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    handler = logging.FileHandler(\"log.txt\", mode='a', encoding=None, delay=False)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)        \n",
    "        \n",
    "#Execution Function\n",
    "def BeerAdvocateScrap(limit=True):\n",
    "    '''\n",
    "    This function executes the BeerAdvocate.com scrapping job.\n",
    "    \n",
    "    Paramters:\n",
    "        Limit = Should the function only scrape the first three style links to test (if True) or all styles (if False). \n",
    "                Default = True.\n",
    "    '''\n",
    "    \n",
    "    #Create directory for beer images\n",
    "    file_path = \"/BeerImages\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory) \n",
    "\n",
    "    \n",
    "    #Create error log\n",
    "    error_log()\n",
    "    \n",
    "    #Get all style links\n",
    "    style_links = Beer_Style_Links()\n",
    "    print(str(len(style_links)) + \" Total Style Links\") #print total styles\n",
    "    \n",
    "    #Get beer links\n",
    "    if limit:\n",
    "        beer_links = []\n",
    "        print(\"Limited to first 3 styles\")\n",
    "        for i in range(4):\n",
    "            Beer_Links(style_links[i], 4, beer_links)\n",
    "    else:\n",
    "        beer_links = []\n",
    "        print(\"Collecting all Styles\")\n",
    "        for i in range(len(style_links)-1):\n",
    "            Beer_Links(style_links[i], 4, beer_links)\n",
    "\n",
    "    print(str(len(beer_links)) + \" Total Beer Links\")\n",
    "\n",
    "    #Create csv file to save scrapped data in.\n",
    "    csvFile = open(\"BeerInformation.csv\",'w', newline='')\n",
    "    #Scrape beer links\n",
    "    try:\n",
    "        writer = csv.writer(csvFile)\n",
    "        for i in range(len(beer_links)-1):\n",
    "            if i % 50 == 0:\n",
    "                print(\"Link Number: \" + str(i))\n",
    "            Beer_Info(beer_links[i], writer)    \n",
    "    finally:\n",
    "        #Close csv file.\n",
    "        csvFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the Scrapping job\n",
    "BeerAdvocateScrap(limit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Below is the data cleaning process along with high level data summarization'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Import the data\n",
    "data = pd.read_csv('BeerInformation.csv', names=(1,2,3,4))\n",
    "print(type(data))\n",
    "\n",
    "print()\n",
    "print(data.head())\n",
    "\n",
    "print()\n",
    "print(data.columns)\n",
    "\n",
    "#Subset each column for indiviual processing\n",
    "c1 = data.loc[:,1]\n",
    "c2 = data.loc[:,2]\n",
    "c3 = data.loc[:,3]\n",
    "c4 = data.loc[:,4]\n",
    "\n",
    "#Print top five row of each series\n",
    "print()\n",
    "print(c1[:5])\n",
    "\n",
    "print()\n",
    "print(c2[:5])\n",
    "\n",
    "print()\n",
    "print(c3[:5])\n",
    "\n",
    "print()\n",
    "print(c4[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean C1: Beer Info\n",
    "\n",
    "#Standardize deliminters\n",
    "regex_pat = re.compile(r'([||]+)') \n",
    "c1_1 = c1.str.replace(regex_pat,'||') \n",
    "\n",
    "#Split the text  by delimiter into columns\n",
    "c1_2 = [p.split('||') for p in c1_1.values]\n",
    "\n",
    "#Cast as a dataframe\n",
    "df = pd.DataFrame(c1_2)\n",
    "\n",
    "#select only required columns\n",
    "df1 = df[[3,5,6,7,9]]\n",
    "print(df1.columns)\n",
    "\n",
    "#Clean each column\n",
    "regex_pat = re.compile(r'(Style:)')\n",
    "df1.loc[:,5] = df1.loc[:,5].str.replace(regex_pat,'')\n",
    "\n",
    "regex_pat = re.compile(r'([Alcohol by volume (ABV): %])')\n",
    "df1.loc[:,6] = df1.loc[:,6].str.replace(regex_pat,'')\n",
    "\n",
    "regex_pat = re.compile(r'(Availability:)')\n",
    "df1.loc[:,7] = df1.loc[:,7].str.replace(regex_pat,'')\n",
    "\n",
    "#rename columns\n",
    "df1.columns = ['company', 'style', 'abv', 'availability', 'notes']\n",
    "print(df1.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean C2: Beer Name\n",
    "\n",
    "#Standardize deliminters\n",
    "regex_pat = re.compile(r'([||]+)')\n",
    "c2_1 = c2.str.replace(regex_pat,'||')\n",
    "\n",
    "#Clean data\n",
    "regex_pat = re.compile(r'(b\\')')\n",
    "c2_1 = c2_1.str.replace(regex_pat,'')\n",
    "\n",
    "regex_pat = re.compile(r'(\")')\n",
    "c2_1 = c2_1.str.replace(regex_pat,'')\n",
    "\n",
    "#Split the text  by delimiter into columns\n",
    "\n",
    "c2_1 = [p.split('||') for p in c2_1.values]\n",
    "\n",
    "#Cast as a dataframe\n",
    "df2 = pd.DataFrame(c2_1)\n",
    "\n",
    "#select only required columns\n",
    "df2 = df2[[0,1]]\n",
    "\n",
    "#rename columns\n",
    "df2.columns = ['beer_name', 'brewery']\n",
    "\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean C3: Beer Rating\n",
    "\n",
    "#Standardize deliminters\n",
    "regex_pat = re.compile(r'([||]+)')\n",
    "c3_1 = c3.str.replace(regex_pat,'||')\n",
    "\n",
    "#Clean each column\n",
    "regex_pat = re.compile(r'(b\\')')\n",
    "c3_1 = c3_1.str.replace(regex_pat,'')\n",
    "\n",
    "regex_pat = re.compile(r'([/5])')\n",
    "c3_1 = c3_1.str.replace(regex_pat,'')\n",
    "\n",
    "regex_pat = re.compile(r'(Ratings)')\n",
    "c3_1 = c3_1.str.replace(regex_pat,'')\n",
    "\n",
    "regex_pat = re.compile(r'([,])')\n",
    "c3_1 = c3_1.str.replace(regex_pat,'')\n",
    "\n",
    "#Split the text  by delimiter into columns\n",
    "c3_1 = [p.split('||') for p in c3_1.values]\n",
    "\n",
    "#Cast as a dataframe\n",
    "df3 = pd.DataFrame(c3_1)\n",
    "\n",
    "#select only required columns\n",
    "df3 = df3[[2,3,4]]\n",
    "\n",
    "#rename columns\n",
    "df3.columns = ['rating','rating_cat', 'number_rating']\n",
    "\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean C2: Beer Stats\n",
    "\n",
    "#Standardize deliminters\n",
    "regex_pat = re.compile(r'([||]+)')\n",
    "c4_1 = c4.str.replace(regex_pat,'||')\n",
    "\n",
    "#Clean each column\n",
    "regex_pat = re.compile(r'(b\\')')\n",
    "c4_1 = c4_1.str.replace(regex_pat,'')\n",
    "\n",
    "regex_pat = re.compile(r'([#,%])')\n",
    "c4_1 = c4_1.str.replace(regex_pat,'')\n",
    "\n",
    "#Split the text  by delimiter into columns\n",
    "c4_1 = [p.split('||') for p in c4_1.values]\n",
    "\n",
    "#Cast as a dataframe\n",
    "df4 = pd.DataFrame(c4_1)\n",
    "\n",
    "#select only required columns\n",
    "df4 = df4[[3,5,7,9,11,15,17,19]]\n",
    "\n",
    "#rename columns\n",
    "df4.columns = ['ranking','reviews', 'ratings','pdev','bro_score','wants','gots','trade']\n",
    "\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the four dataframes and cast columns to numeric as appropriate\n",
    "df = pd.concat([df1, df2, df3, df4], axis=1)\n",
    "\n",
    "#Cast numeric columns to numeric from string\n",
    "df['abv'] = pd.to_numeric(df['abv'], errors='coerce')\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "df['number_rating'] = pd.to_numeric(df['number_rating'], errors='coerce')\n",
    "df['ranking'] = pd.to_numeric(df['ranking'], errors='coerce')\n",
    "df['reviews'] = pd.to_numeric(df['reviews'], errors='coerce')\n",
    "df['ratings'] = pd.to_numeric(df['ratings'], errors='coerce')\n",
    "df['pdev'] = pd.to_numeric(df['pdev'], errors='coerce')\n",
    "df['bro_score'] = pd.to_numeric(df['bro_score'], errors='coerce')\n",
    "df['wants'] = pd.to_numeric(df['wants'], errors='coerce')\n",
    "df['gots'] = pd.to_numeric(df['gots'], errors='coerce')\n",
    "df['trade'] = pd.to_numeric(df['trade'], errors='coerce')\n",
    "\n",
    "\n",
    "print(df.head(2))\n",
    "print()\n",
    "\n",
    "print(df.columns)\n",
    "print()\n",
    "\n",
    "print(df.shape)\n",
    "print()\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Summaries'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts by Style\n",
    "counts = df['style'].value_counts()\n",
    "\n",
    "print(\"top 20 beer styles by beer count\")\n",
    "print(counts[:20])\n",
    "\n",
    "print()\n",
    "print(\"bottom 20 beer styles by beer count\")\n",
    "print(counts[-20:])\n",
    "\n",
    "'''\n",
    "By stype, the top beers appear to all have the same general count. This is expected as the web scrapper only took \n",
    "the first four pages of the style. The depth of the scrape would need to be set deeper to determine the beer with the \n",
    "most beer styles.\n",
    "\n",
    "For the beers with the least styles however, just scrapping the first four pages did provide enough information. \n",
    "The beer styles with the least beers listed include :\n",
    " - Faro, Happoshu, Sahti, Black and Tan, and Eisbock\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts by Company\n",
    "counts = df['company'].value_counts()\n",
    "\n",
    "print('Top 20 companies by beer count')\n",
    "print(counts[:20])\n",
    "\n",
    "'''\n",
    "Of the beers listed on the first four pages of the styles, Bostom Beer Company has the most listed.\n",
    "'''\n",
    "\n",
    "print()\n",
    "print('Bottom 20 companies by beer count')\n",
    "print(counts[-20:])\n",
    "\n",
    "'''\n",
    "There are many companies with only one beer listed.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts by Availability\n",
    "counts = df['availability'].value_counts()\n",
    "\n",
    "print('Beer Availability')\n",
    "print(counts)\n",
    "\n",
    "'''\n",
    "Most beers scrapped are offered year round. A large number of beers are offered on a rotating basis.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts by Rating Category\n",
    "counts = df['rating_cat'].value_counts()\n",
    "\n",
    "print(\"Beer Ratings (categorical)\")\n",
    "print(counts)\n",
    "\n",
    "counts = df['rating_cat'].value_counts(normalize=True)\n",
    "\n",
    "'''\n",
    "Only 204 (1%) beers are reated as world class. 50% of beers are rated as good/very good.\n",
    "'''\n",
    "\n",
    "print()\n",
    "print(\"Beer Ratings (categorical)\")\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary stats by abv\n",
    "print(\"Summary of Alcohol By Volume\")\n",
    "print(df['abv'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary stats by rating\n",
    "print(\"Summary by Rating (out of 5)\")\n",
    "print(df['rating'].describe())\n",
    "\n",
    "'''\n",
    "The mean rating is 3.6 and the median is 3.74 out of 5.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary stats by number of ratings\n",
    "print('Summary of the number of ratings')\n",
    "print(df['number_rating'].describe())\n",
    "\n",
    "'''\n",
    "The mean number of ratings is 226 per beer while the median is only 38. The number of ratings is very scewed with the \n",
    "beer with the max number of ratings having over 16K ratings.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary stats by abv\n",
    "print(\"Summary of number of Reviews\")\n",
    "print(df['reviews'].describe())\n",
    "\n",
    "'''\n",
    "The number of reviews is right skewed. The mean is higher than the median. Most beers get only a handful of reviews (17) or\n",
    "less though a handful get a lot (over 100 or even 1000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary stats by rating\n",
    "print(\"Summary of Ratings\")\n",
    "print(df['ratings'].describe())\n",
    "\n",
    "'''\n",
    "The number of reviews is right skewed. The mean is higher than the median. Most beers get only a handful of reviews (55) or\n",
    "less though a handful get a lot (over 200 or even 1000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted Average Rating by Style\n",
    "grouped = df.groupby('style')\n",
    "\n",
    "def wavg(group):\n",
    "    d = group['rating']\n",
    "    w = group['ratings']\n",
    "    return (d * w).sum() / w.sum()\n",
    "\n",
    "wa = grouped.apply(wavg).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 Styles\")\n",
    "print(wa[:20])\n",
    "\n",
    "print()\n",
    "print(\"Bottom 20 Styles\")\n",
    "print(wa[-20:])\n",
    "\n",
    "'''\n",
    "The top rated styles have on average a score of 4 or more. While the lowest rated styles have a rating of 3 or less.\n",
    "\n",
    "The top rated styles are Imperial Stout/Gueze, Imperial IPA, and Wild Ale.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted Average Rating by Company\n",
    "grouped = df.groupby('company')\n",
    "\n",
    "def wavg(group):\n",
    "    d = group['rating']\n",
    "    w = group['ratings']+.00000000000000000000000001\n",
    "    return (d * w).sum() / w.sum()\n",
    "\n",
    "wa = grouped.apply(wavg).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 Company\")\n",
    "print(wa[:20])\n",
    "\n",
    "print()\n",
    "print(\"Bottom 20 Company\")\n",
    "print(wa[-20:])\n",
    "\n",
    "'''\n",
    "The top rate companies all appear to be craft breweries as non of the main brands appear in the top 20. For the \n",
    "lowest rated companies, they all have 0 ratings.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
